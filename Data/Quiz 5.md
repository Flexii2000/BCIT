# Quiz
### Question 1 (1 point) 
 Saved
Mean Squared Error (MSE) is more sensitive to outliers than Mean Absolute Error (MAE).

Question 1 options:
	True
	False

### Question 2 (1 point) 
 Saved
A high accuracy in classification always indicates a good model.

Question 2 options:
	True
	False

### Question 3 (1 point) 
 Saved
What happens when k is too small in k-Nearest Neighbors?

Question 3 options:

The model becomes too sensitive to noise and overfits the data.


The decision boundary becomes smoother.


The model generalizes well to unseen data.


The model fails to make predictions.

### Question 4 (1 point) 
 Saved
In univariate statistical analysis, the standard deviation measures the central tendency of the data.

Question 4 options:
	True
	False
	
### Question 5 (1 point) 
 Saved
R-squared (R²) can be negative.

Question 5 options:
	True
	False

### Question 6 (1 point) 
 Saved
A model with high specificity but low sensitivity is likely to miss many actual positive cases.

Question 6 options:
	True
	False

### Question 7 (1 point) 
 Saved
Which of the following is finally produced by Hierarchical Clustering?

Question 7 options:

Final estimate of cluster centroids


Tree showing how close things are to each other


Assignment of each point to clusters


All of the mentioned

### Question 8 (1 point) 
 Saved
In supervised learning, which of the following is NOT a characteristic feature?

Question 8 options:

Requires labeled training data


Can detect hidden patterns in unlabeled data


Used for classification and regression tasks


Uses a mapping function to predict outputs

### Question 9 (1 point) 
 Saved
Which of the following best describes how a Random Forest classifier improves over a single Decision Tree?

Question 9 options:

It reduces overfitting by averaging multiple decision trees trained on different subsets of the data.


It uses a more complex tree structure with additional layers.


It trains multiple trees using the entire dataset.


It selects the best decision tree out of many rather than averaging them. 

### Question 10 (1 point) 
 Saved
Which of the following classifiers does NOT explicitly create a decision boundary?

Question 10 options:

 Decision Tree


k-Nearest Neighbors (k-NN)


Logistic Regression


Support Vector Machine (SVM)

### Question 11 (1 point) 
Which of the following statements about the ROC curve is TRUE?

Question 11 options:

A perfect classifier will have an ROC curve that passes through the top-left corner (1,1).


The diagonal line (y = x) in the ROC curve represents a perfect classifier.


The area under the ROC curve (AUC) is always between 0.5 and 1.


A model with an AUC of 0.7 is better than a model with an AUC of 0.9.

### Question 12 (1 point) 
 Saved
Which metric is most appropriate when dealing with imbalanced classification problems?

Question 12 options:

 R-squared


Mean Squared Error (MSE)


F1-score


Accuracy

### Question 13 (1 point) 
 Saved
Precision is defined as the proportion of correctly predicted positive cases out of all actual positive cases.

Question 13 options:
	True
	False

### Question 14 (1 point) 
 Saved
What is the main difference between logistic and linear regression?

Question 14 options:

Logistic regression models continuous outcomes, whereas linear regression models categorical outcomes.


Linear regression works only for classification problems.


Logistic regression does not assume any relationship between the input and output variables.


Logistic regression predicts probabilities using a sigmoid function, whereas linear regression predicts continuous values.

### Question 15 (1 point) 
Which statement about the F1-score is TRUE?

Question 15 options:

It is the harmonic mean of precision and recall.


It is always higher than both precision and recall.


It can be used only for binary classification.


A high F1-score always means high accuracy.

### Question 16 (1 point) 
Which of the following statements about confidence intervals is TRUE?

Question 16 options:

A wider confidence interval indicates a more precise estimate.


The confidence level represents the probability that the sample mean equals the population mean.


Increasing the sample size generally leads to a narrower confidence interval.


Confidence intervals can only be constructed for normally distributed data.

### Question 17 (1 point) 
The ROC-AUC score is useful even when class distributions are highly imbalanced.

Question 17 options:
	True
	False

### Question 18 (1 point) 
 Saved
The ROC curve plots precision against recall.

Question 18 options:
	True
	False

### Question 19 (1 point) 
 Saved
Sensitivity and specificity always increase together.

Question 19 options:
	True
	False

### Question 20 (1 point) 
 Saved
Which statistical method is used to assess the reliability of an estimate?

Question 20 options:

Hypothesis Testing


Confidence Interval Estimation


Regression Analysis


Principal Component Analysis

### Question 21 (1 point) 
 Saved
The k-NN classifier gets computationally expensive during training.

Question 21 options:
	True
	False

### Question 22 (1 point) 
 Saved
Which of the following is required by K-means clustering?

Question 22 options:

Defined distance metric


Number of clusters


Initial guess as to cluster centroids


All of the mentioned

### Question 23 (1 point) 
 Saved
Which of the following is a valid reason why supervised learning might fail?

Question 23 options:

The model was trained with labeled data


The test data contains patterns the model has never seen before


The model was built using a large dataset


The model learns the general patterns instead of memorizing training data

### Question 24 (1 point) 
 Saved
Which of the following assumptions is NOT required for linear regression?

Question 24 options:

Independence of observations


Homoscedasticity of residuals


The dependent variable must be categorical


Linearity of the relationship between independent and dependent variables

### Question 25 (1 point) 
 Saved
Which statement about Support Vector Machines (SVM) is FALSE?

Question 25 options:

A kernel trick allows SVMs to handle non-linearly separable data.


The support vectors are the critical data points that define the decision boundary.


SVMs try to maximize the margin between classes.


A linear SVM always performs better than a k-NN classifier.

### Question 26 (1 point) 
 Saved
Which metric is generally NOT used for evaluating regression models?

Question 26 options:

F1-score


Mean Absolute Error (MAE)


Root Mean Squared Error (RMSE)


R-squared (R²)

### Question 27 (1 point) 
 Saved
Which of the following is a disadvantage of k-fold cross-validation?

Question 27 options:

It is computationally expensive for large datasets.


It always provides a lower variance estimate compared to a single train-test split.


 It cannot be used for small datasets.


It always leads to overfitting.


23 of 27 questions saved